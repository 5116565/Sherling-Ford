# 随机森林实战演练

>本代码演练的是决策树算法，以及随机森林在内的机器学习集成算法。

## 参数解读

Bagging框架的参数和GBDT对比，GBDT的框架参数比较多，重要的有最大迭代器的个数，步长和子采样的比例等，调参过程比较费力。但是基于Bagging框架的随机森林则比较简单，这是因为Bagging的各个弱学习器之间是没有依赖关系的，降低了调参的难度。换句话说，达到同样的调参效果，随机森林调参时间要比GBDT少一些。

下面我们来看看随机森林的重要参数，由于Random Forest Classifier和Random Forest Regressor参数绝大部分相同，这里会将它们一起讲，不同点会指出。

  n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的决策树个数，默认是10。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。

  bootstrap：默认True，是否有放回的采样。
  
  oob_score：默认识False，即是否采用袋外样本来评估模型的好坏。有放回采样中大约会有36.8%的数据没有被采样到，我们常常称之为袋外数据(Out Of Bag, 简称OOB)，这些数据没有参与到训练集模型的拟合，因此可以用来检测模型的泛化能力。推荐设置为True，因为袋外分数反应了模型拟合后的泛化能力。对单个模型的参数训练，我们知道可以用cross validation（cv）来进行，但是比较消耗时间，而且对于随机森林这种情况没有太大的必要，所以就用袋外数据对决策树模型进行验证，类似一个简单的交叉验证，性能消耗小，但是效果不错。  

  criterion：
  即CART树做划分时对特征的评价标准，分类模型和回归模型的损失函数是不一样的。

  （1）分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益entropy，是用来选择节点的最优特征和切分点的两个准则。

  （2）回归RF对应的CART回归树默认是均方差MSE，另一个可选择的标准是绝对值误差MAE。一般来说选择默认的标准就已经很好的。
  
  max_features: 随机森林划分时考虑的最大特征数。可以使用很多种类型的值，默认是"None",意味着划分时考虑所有的特征数；如果是"log2"意味着划分时最多考虑log2N个特征；如果是"sqrt"或者"auto"意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数，其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的"None"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。
  
  max_depth:
  决策树最大深度。默认为"None"，即不限制子树的深度。这样建树时，会使每一个叶节点只有一个类别，或是达到min_samples_split。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。

  min_samples_split:
  内部节点再划分所需的最小样本数，默认2。这个值限制了子树继续划分的条件，如果某节点的样本数小于min_samples_split，则不会继续划分。如果样本量不大，可以不设置这个值；如果样本量数量级非常大，则推荐增大这个值。

  min_samples_leaf: 叶子节点最少样本数。
  这个值限制了叶子节点的最小样本数，如果某个叶子节点的样本数小于min_samples_leaf，则会和兄弟节点一起被剪枝。默认是1，可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。

  min_weight_fraction_leaf: 叶子节点最小的样本权重和。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。
  默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。

  max_leaf_nodes:
  最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合。默认是"None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。

  min_impurity_split: 
  节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点，即为叶子节点。一般不推荐改动默认值1e-7。

  splitter:
  随机选择属性"random"还是选择不纯度最大"best"的属性，建议用默认best。

  presort: 是否对数据进行预分类，以加快拟合中最佳分裂点的发现。默认False，适用于大数据集。小数据集使用True,可以加快训练。是否预排序,预排序可以加速查找最佳分裂点，对于稀疏数据不管用，Bool，auto：非稀疏数据则预排序，若稀疏数据则不预排序。

## 常用方法

>predict_proba(x)：给出带有概率值的结果。每个点在所有label（类别）的概率和为1。
> predict(x)：直接给出预测结果。内部还是调用的predict_proba()，根据概率的结果看哪个类型的预测值最高就是哪个类型。
> predict_log_proba(x)：和predict_proba基本上一样，只是把结果给做了log()处理。

## 代码实践

```Python
import numpy as np
import scipy
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import roc_curve, auc, roc_auc_score

import matplotlib.pyplot as plt
df=pd.read_csv('D:\\BaiduNetdiskDownload\\case-random forest.csv', encoding='gbk')
df.head()
df.isrun.nunique()
df.isrun.value_counts()
df.isrun=df.isrun.astype(str).map({'False.':0, 'True.':1})
y=df.isrun
y.head()
x=df.drop('isrun', axis=1)   #dataframe.drop('isrun', axis=1)
x.head()
seed=5
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=seed)
```

### 模型训练

```Python
rfc = RandomForestClassifier()     #实例化
rfc = rfc.fit(xtrain,ytrain)       #用训练集数据训练模型 

result = rfc.score(xtest,ytest)    #导入测试集，rfc的接口score计算的是模型准确率accuracy
result
print ('所有的树:%s' % rfc.estimators_)
print (rfc.classes_)
print (rfc.n_classes_)
```

### 预测结果

```Python
print ('判定结果：%s' % rfc.predict(xtest))
#print rfc.predict_proba(feature_test[0])
print ('判定结果：%s' % rfc.predict_proba(xtest)[:,:])   #标签是1的可能性
print ('判定结果：%s' % rfc.predict_proba(xtest)[:,1])   #标签是1的可能性
#对比一下，（1）使用predict_proba()接口，然后将标签值为1的概率>0.5的样本即判断其属于标签值为1的那一类；
#（2）直接使用predict接口，判断其属于哪一类
d1=np.array(pd.Series(rfc.predict_proba(xtest)[:,1]>0.5).map({False:0, True:1}))
d2=rfc.predict(xtest)
np.array_equal(d1,d2)
roc_auc_score(ytest, rfc.predict_proba(xtest)[:,1])
```

### 特征的重要性与可视化

```Python
print ('各feature的重要性：%s' % rfc.feature_importances_)
importances = rfc.feature_importances_
std = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis=0)
indices = np.argsort(importances)[::-1]# Print the feature ranking
print("Feature ranking:")
for f in range(min(20,xtrain.shape[1])):
    print("%2d) %-*s %f" % (f + 1, 30, xtrain.columns[indices[f]], importances[indices[f]]))# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(xtrain.shape[1]), importances[indices],  color="r", yerr=std[indices], align="center")
plt.xticks(range(xtrain.shape[1]), indices)
plt.xlim([-1, xtrain.shape[1]])
plt.show()
tuple(xtrain.columns[indices])
f, ax = plt.subplots(figsize=(7, 5))
ax.bar(range(len(rfc.feature_importances_)), rfc.feature_importances_)
ax.set_title("Feature Importances")
plt.show()
predictions_validation = rfc.predict_proba(xtest)[:,1]
fpr, tpr, _ = roc_curve(ytest, predictions_validation)
roc_auc = auc(fpr, tpr)
plt.title('ROC Validation')
plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)
plt.legend(loc='lower right')
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()
```

### 交叉验证

```Python
sklearn.model_selection.cross_val_score(estimator, X, y, scoring=None,
cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’)
```

#### 参数解析

estimator:   估计方法对象(分类器)

X：        数据特征(Features)

y：        数据标签(Labels)

soring：    调用方法(包括accuracy和mean_squared_error等等)

cv：       几折交叉验证

n_jobs：   同时工作的cpu个数（-1代表全部）

```Python
clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)
scores = cross_val_score(clf, xtrain, ytrain)
print(scores.mean())

clf2 = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)
scores = cross_val_score(clf2, xtrain, ytrain)
print(scores.mean())
```

### 网格搜索最优的超参数值

```Python
rfc.get_params
param_test1 = {'n_estimators': range(25,500,25)}
gsearch1 = GridSearchCV(estimator = RandomForestClassifier(min_samples_split=100,
                                                           min_samples_leaf=20,
                                                           max_depth=8, random_state=10),
                        param_grid = param_test1,
                        scoring='roc_auc',
                        cv=5)
gsearch1.fit(xtrain, ytrain)
print(gsearch1.best_params_, gsearch1.best_score_)
# gsearch1.cv_results_打印拟合结果)
param_test2 = {'min_samples_split':range(60, 200, 20), 'min_samples_leaf':range(10, 110, 10)}
gsearch2 = GridSearchCV(estimator = RandomForestClassifier(n_estimators=300,
                                                           max_depth=8, random_state=10), 
                        param_grid = param_test2,
                        scoring='roc_auc',
                        cv=5)
gsearch2.fit(xtrain,ytrain)
print(gsearch2.best_params_, gsearch2.best_score_)
param_test3 = {'max_depth':range(3, 30, 2)}
gsearch3 = GridSearchCV(estimator = RandomForestClassifier(n_estimators=300,
                                                           min_samples_split=60,
                                                           min_samples_leaf=10,
                                                           random_state=10),
                        param_grid = param_test3,
                        scoring='roc_auc',
                        cv=5)
gsearch3.fit(xtrain,ytrain)
print(gsearch3.best_params_, gsearch3.best_score_)
roc_auc_score(ytest, gsearch3.best_estimator_.predict_proba(xtest)[:,1])
gsearch3.best_estimator_
param_test4 = {'criterion':['gini', 'entropy'], 'class_weight':[None, 'balanced']}
gsearch4 = GridSearchCV(estimator = RandomForestClassifier(n_estimators=300,
                                                           max_depth=7,
                                                           min_samples_split=60,
                                                           min_samples_leaf=10,
                                                           random_state=10),
                        param_grid = param_test4,
                        scoring='roc_auc',
                        cv=5)
gsearch4.fit(xtrain,ytrain)
print(gsearch4.best_params_, gsearch4.best_score_)
roc_auc_score(ytest, gsearch4.best_estimator_.predict_proba(xtest)[:,1])
```

### 运行结果

所有的树:[DecisionTreeClassifier(max_features='auto', random_state=541424326), DecisionTreeClassifier(max_features='auto', random_state=1386472813), DecisionTreeClassifier(max_features='auto', random_state=1359274595), DecisionTreeClassifier(max_features='auto', random_state=337532538), DecisionTreeClassifier(max_features='auto', random_state=1416261744), DecisionTreeClassifier(max_features='auto', random_state=1554418458), DecisionTreeClassifier(max_features='auto', random_state=1506643143), DecisionTreeClassifier(max_features='auto', random_state=403750210), DecisionTreeClassifier(max_features='auto', random_state=709687146), DecisionTreeClassifier(max_features='auto', random_state=119026059), DecisionTreeClassifier(max_features='auto', random_state=1827604204), DecisionTreeClassifier(max_features='auto', random_state=318141960), DecisionTreeClassifier(max_features='auto', random_state=797873334), DecisionTreeClassifier(max_features='auto', random_state=1318551480), DecisionTreeClassifier(max_features='auto', random_state=1092986288), DecisionTreeClassifier(max_features='auto', random_state=1950595209), DecisionTreeClassifier(max_features='auto', random_state=2145305763), DecisionTreeClassifier(max_features='auto', random_state=973599419), DecisionTreeClassifier(max_features='auto', random_state=1481953957), DecisionTreeClassifier(max_features='auto', random_state=1254509944), DecisionTreeClassifier(max_features='auto', random_state=874934689), DecisionTreeClassifier(max_features='auto', random_state=1762394474), DecisionTreeClassifier(max_features='auto', random_state=1036492757), DecisionTreeClassifier(max_features='auto', random_state=1300386255), DecisionTreeClassifier(max_features='auto', random_state=1643501373), DecisionTreeClassifier(max_features='auto', random_state=2146636235), DecisionTreeClassifier(max_features='auto', random_state=1089070730), DecisionTreeClassifier(max_features='auto', random_state=1131007726), DecisionTreeClassifier(max_features='auto', random_state=1635542618), DecisionTreeClassifier(max_features='auto', random_state=544739587), DecisionTreeClassifier(max_features='auto', random_state=1296984604), DecisionTreeClassifier(max_features='auto', random_state=214804243), DecisionTreeClassifier(max_features='auto', random_state=2113362945), DecisionTreeClassifier(max_features='auto', random_state=1279155361), DecisionTreeClassifier(max_features='auto', random_state=1973306042), DecisionTreeClassifier(max_features='auto', random_state=603422475), DecisionTreeClassifier(max_features='auto', random_state=947953749), DecisionTreeClassifier(max_features='auto', random_state=1856614955), DecisionTreeClassifier(max_features='auto', random_state=896893789), DecisionTreeClassifier(max_features='auto', random_state=1674241617), DecisionTreeClassifier(max_features='auto', random_state=423050117), DecisionTreeClassifier(max_features='auto', random_state=952205425), DecisionTreeClassifier(max_features='auto', random_state=963267450), DecisionTreeClassifier(max_features='auto', random_state=876106049), DecisionTreeClassifier(max_features='auto', random_state=173696818), DecisionTreeClassifier(max_features='auto', random_state=2088415320), DecisionTreeClassifier(max_features='auto', random_state=909503052), DecisionTreeClassifier(max_features='auto', random_state=1510950937), DecisionTreeClassifier(max_features='auto', random_state=1512400486), DecisionTreeClassifier(max_features='auto', random_state=548814146), DecisionTreeClassifier(max_features='auto', random_state=735677618), DecisionTreeClassifier(max_features='auto', random_state=1708302754), DecisionTreeClassifier(max_features='auto', random_state=1084352238), DecisionTreeClassifier(max_features='auto', random_state=1879461609), DecisionTreeClassifier(max_features='auto', random_state=1985500396), DecisionTreeClassifier(max_features='auto', random_state=1634389901), DecisionTreeClassifier(max_features='auto', random_state=2040071836), DecisionTreeClassifier(max_features='auto', random_state=1082922304), DecisionTreeClassifier(max_features='auto', random_state=2031815488), DecisionTreeClassifier(max_features='auto', random_state=1253037347), DecisionTreeClassifier(max_features='auto', random_state=619290935), DecisionTreeClassifier(max_features='auto', random_state=1096962892), DecisionTreeClassifier(max_features='auto', random_state=2002273114), DecisionTreeClassifier(max_features='auto', random_state=844310893), DecisionTreeClassifier(max_features='auto', random_state=965900510), DecisionTreeClassifier(max_features='auto', random_state=1079111125), DecisionTreeClassifier(max_features='auto', random_state=647552482), DecisionTreeClassifier(max_features='auto', random_state=396676203), DecisionTreeClassifier(max_features='auto', random_state=1907935760), DecisionTreeClassifier(max_features='auto', random_state=528563397), DecisionTreeClassifier(max_features='auto', random_state=1136036398), DecisionTreeClassifier(max_features='auto', random_state=185532493), DecisionTreeClassifier(max_features='auto', random_state=154114434), DecisionTreeClassifier(max_features='auto', random_state=1262041618), DecisionTreeClassifier(max_features='auto', random_state=733215875), DecisionTreeClassifier(max_features='auto', random_state=1151424419), DecisionTreeClassifier(max_features='auto', random_state=2002958067), DecisionTreeClassifier(max_features='auto', random_state=575049633), DecisionTreeClassifier(max_features='auto', random_state=1666366952), DecisionTreeClassifier(max_features='auto', random_state=1884029910), DecisionTreeClassifier(max_features='auto', random_state=359763805), DecisionTreeClassifier(max_features='auto', random_state=192684385), DecisionTreeClassifier(max_features='auto', random_state=1855158832), DecisionTreeClassifier(max_features='auto', random_state=153673080), DecisionTreeClassifier(max_features='auto', random_state=1158937872), DecisionTreeClassifier(max_features='auto', random_state=959480274), DecisionTreeClassifier(max_features='auto', random_state=711671122), DecisionTreeClassifier(max_features='auto', random_state=1265015815), DecisionTreeClassifier(max_features='auto', random_state=1374742511), DecisionTreeClassifier(max_features='auto', random_state=1245497171), DecisionTreeClassifier(max_features='auto', random_state=1446949142), DecisionTreeClassifier(max_features='auto', random_state=1658776825), DecisionTreeClassifier(max_features='auto', random_state=1466630027), DecisionTreeClassifier(max_features='auto', random_state=1579044378), DecisionTreeClassifier(max_features='auto', random_state=1891955939), DecisionTreeClassifier(max_features='auto', random_state=1722229382), DecisionTreeClassifier(max_features='auto', random_state=414121871), DecisionTreeClassifier(max_features='auto', random_state=8199386), DecisionTreeClassifier(max_features='auto', random_state=2021086682), DecisionTreeClassifier(max_features='auto', random_state=1837712314)]
[0 1]
2
判定结果：
[0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0
 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0
 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
 0]
判定结果：[[0.98 0.02]
 [0.98 0.02]
 [0.97 0.03]
 ...
 [1.   0.  ]
 [0.89 0.11]
 [0.97 0.03]]
判定结果：
[0.02 0.02 0.03 0.03 0.05 0.87 0.93 0.7  0.22 0.15 0.13 0.07 0.93 0.1
 0.62 0.04 0.02 0.01 0.24 0.14 0.   0.   0.06 0.05 0.25 0.04 0.03 0.32
 0.04 0.09 0.11 0.16 0.03 0.12 0.   0.1  0.03 0.82 0.01 0.05 0.02 0.02
 0.08 0.06 0.1  0.18 0.04 0.16 0.07 0.09 0.52 0.17 0.04 0.35 0.08 0.07
 0.17 0.03 0.58 0.02 0.05 0.02 0.62 0.04 0.03 0.11 0.07 0.42 0.82 0.39
 0.01 0.14 0.46 0.03 0.09 0.32 0.15 0.06 0.06 0.15 0.07 0.   0.14 0.04
 0.07 0.75 0.01 0.03 0.34 0.22 0.08 0.07 0.03 0.27 0.05 0.42 0.55 0.02
 0.07 0.18 0.49 0.21 0.06 0.1  0.57 0.12 0.11 0.13 0.01 0.22 0.16 0.28
 0.02 0.02 0.   0.26 0.02 0.01 0.35 0.01 0.09 0.08 0.09 0.07 0.05 0.19
 0.15 0.64 0.08 0.03 0.17 0.13 0.86 0.07 0.07 0.21 0.46 0.43 0.09 0.08
 0.13 0.14 0.09 0.03 0.32 0.13 0.36 0.03 0.21 0.04 0.42 0.24 0.1  0.1
 0.83 0.05 0.   0.34 0.04 0.02 0.04 0.23 0.86 0.36 0.09 0.19 0.41 0.17
 0.04 0.18 0.01 0.79 0.02 0.05 0.17 0.05 0.05 0.09 0.04 0.19 0.05 0.03
 0.05 0.19 0.04 0.1  0.06 0.01 0.08 0.15 0.   0.16 0.07 0.06 0.09 0.18
 0.31 0.02 0.16 0.01 0.05 0.09 0.12 0.1  0.19 0.03 0.01 0.14 0.03 0.78
 0.1  0.1  0.06 0.19 0.16 0.13 0.3  0.15 0.56 0.   0.05 0.04 0.13 0.7
 0.09 0.03 0.   0.24 0.58 0.1  0.56 0.68 0.01 0.18 0.12 0.02 0.07 0.04
 0.03 0.1  0.3  0.2  0.01 0.09 0.43 0.1  0.13 0.1  0.82 0.11 0.12 0.18
 0.04 0.28 0.01 0.01 0.86 0.77 0.04 0.11 0.08 0.06 0.08 0.67 0.09 0.22
 0.04 0.08 0.05 0.04 0.02 0.12 0.74 0.01 0.25 0.1  0.02 0.65 0.02 0.07
 0.02 0.11 0.06 0.14 0.67 0.11 0.09 0.01 0.03 0.73 0.02 0.05 0.11 0.44
 0.03 0.15 0.13 0.   0.01 0.44 0.01 0.05 0.34 0.08 0.05 0.03 0.   0.02
 0.17 0.16 0.06 0.33 0.08 0.94 0.03 0.02 0.52 0.89 0.02 0.11 0.01 0.17
 0.88 0.3  0.06 0.08 0.27 0.08 0.46 0.03 0.53 0.   0.03 0.21 0.1  0.1
 0.47 0.04 0.27 0.01 0.13 0.33 0.01 0.06 0.11 0.02 0.05 0.03 0.12 0.06
 0.02 0.93 0.18 0.02 0.11 0.06 0.01 0.01 0.07 0.09 0.04 0.24 0.33 0.63
 0.02 0.09 0.1  0.01 0.15 0.01 0.02 0.05 0.01 0.35 0.06 0.05 0.04 0.12
 0.08 0.07 0.07 0.14 0.02 0.1  0.02 0.07 0.02 0.05 0.05 0.43 0.63 0.02
 0.32 0.07 0.11 0.12 0.02 0.07 0.17 0.17 0.   0.04 0.14 0.01 0.02 0.03
 0.51 0.1  0.13 0.02 0.04 0.04 0.07 0.3  0.13 0.3  0.14 0.5  0.95 0.12
 0.07 0.57 0.03 0.64 0.04 0.54 0.04 0.08 0.02 0.04 0.04 0.56 0.02 0.18
 0.01 0.13 0.21 0.03 0.03 0.9  0.14 0.21 0.06 0.04 0.08 0.06 0.   0.06
 0.15 0.06 0.09 0.17 0.26 0.15 0.01 0.11 0.12 0.07 0.   0.04 0.01 0.02
 0.02 0.13 0.02 0.05 0.14 0.12 0.04 0.15 0.06 0.03 0.14 0.18 0.25 0.02
 0.32 0.48 0.04 0.04 0.77 0.36 0.11 0.22 0.16 0.52 0.01 0.09 0.02 0.07
 0.04 0.19 0.16 0.05 0.1  0.02 0.04 0.34 0.53 0.   0.19 0.2  0.11 0.02
 0.13 0.64 0.26 0.06 0.   0.15 0.01 0.05 0.05 0.07 0.14 0.02 0.16 0.13
 0.32 0.69 0.02 0.24 0.21 0.14 0.03 0.03 0.05 0.09 0.04 0.11 0.02 0.1
 0.06 0.11 0.09 0.23 0.11 0.24 0.06 0.07 0.03 0.12 0.2  0.03 0.   0.41
 0.   0.07 0.78 0.07 0.1  0.17 0.09 0.02 0.12 0.14 0.01 0.63 0.05 0.02
 0.88 0.37 0.19 0.1  0.08 0.39 0.76 0.31 0.16 0.27 0.61 0.02 0.06 0.25
 0.   0.06 0.08 0.31 0.02 0.77 0.03 0.2  0.16 0.01 0.05 0.04 0.1  0.02
 0.14 0.05 0.19 0.01 0.02 0.09 0.03 0.09 0.61 0.09 0.04 0.06 0.03 0.89
 0.06 0.   0.04 0.83 0.04 0.07 0.02 0.12 0.01 0.03 0.72 0.01 0.05 0.71
 0.04 0.03 0.13 0.24 0.09 0.06 0.05 0.16 0.05 0.15 0.07 0.2  0.01 0.01
 0.38 0.06 0.02 0.24 0.08 0.07 0.01 0.82 0.07 0.04 0.35 0.01 0.06 0.05
 0.11 0.13 0.02 0.07 0.15 0.7  0.06 0.03 0.02 0.42 0.04 0.04 0.05 0.03
 0.1  0.01 0.08 0.03 0.03 0.03 0.03 0.11 0.58 0.07 0.73 0.64 0.29 0.03
 0.63 0.07 0.02 0.06 0.02 0.03 0.34 0.11 0.05 0.01 0.14 0.02 0.09 0.22
 0.01 0.73 0.55 0.   0.16 0.03 0.08 0.12 0.01 0.14 0.19 0.   0.09 0.
 0.13 0.05 0.08 0.16 0.11 0.24 0.13 0.05 0.05 0.01 0.2  0.06 0.04 0.06
 0.12 0.72 0.2  0.04 0.11 0.04 0.09 0.2  0.07 0.02 0.1  0.05 0.1  0.08
 0.27 0.05 0.09 0.14 0.16 0.21 0.01 0.11 0.8  0.19 0.05 0.01 0.04 0.36
 0.08 0.1  0.01 0.01 0.06 0.1  0.04 0.33 0.03 0.86 0.   0.21 0.06 0.06
 0.02 0.3  0.02 0.83 0.17 0.36 0.75 0.95 0.01 0.04 0.29 0.37 0.09 0.08
 0.03 0.06 0.04 0.02 0.39 0.17 0.08 0.08 0.05 0.29 0.29 0.12 0.03 0.56
 0.69 0.32 0.07 0.13 0.01 0.08 0.17 0.09 0.04 0.07 0.14 0.06 0.23 0.28
 0.05 0.06 0.05 0.03 0.13 0.16 0.72 0.05 0.06 0.15 0.06 0.06 0.04 0.11
 0.07 0.39 0.02 0.05 0.04 0.06 0.01 0.41 0.09 0.02 0.46 0.14 0.07 0.09
 0.08 0.1  0.13 0.05 0.04 0.04 0.79 0.03 0.09 0.12 0.14 0.07 0.13 0.02
 0.02 0.13 0.01 0.12 0.06 0.03 0.02 0.04 0.8  0.05 0.04 0.08 0.13 0.01
 0.07 0.06 0.86 0.68 0.07 0.11 0.38 0.12 0.43 0.2  0.43 0.23 0.12 0.03
 0.   0.04 0.04 0.08 0.08 0.13 0.49 0.32 0.09 0.02 0.01 0.9  0.32 0.05
 0.02 0.1  0.05 0.14 0.1  0.16 0.19 0.01 0.93 0.39 0.31 0.12 0.9  0.15
 0.05 0.09 0.03 0.02 0.05 0.09 0.07 0.09 0.1  0.08 0.06 0.18 0.02 0.06
 0.17 0.32 0.   0.63 0.11 0.08 0.21 0.04 0.1  0.11 0.03 0.02 0.5  0.02
 0.04 0.18 0.11 0.2  0.2  0.26 0.07 0.09 0.09 0.03 0.   0.88 0.26 0.19
 0.08 0.15 0.03 0.13 0.08 0.03 0.62 0.11 0.04 0.07 0.02 0.04 0.09 0.06
 0.01 0.05 0.06 0.04 0.08 0.31 0.04 0.03 0.03 0.08 0.02 0.04 0.02 0.03
 0.   0.02 0.85 0.16 0.02 0.08 0.11 0.28 0.04 0.1  0.28 0.01 0.08 0.14
 0.74 0.01 0.03 0.04 0.02 0.19 0.03 0.02 0.13 0.11 0.04 0.78 0.05 0.06
 0.04 0.04 0.07 0.   0.11 0.03]
各feature的重要性：
[0.04555429 0.15346163 0.04712458 0.14130572 0.08032606 0.04541184
 0.08323822 0.05569061 0.04801155 0.05477623 0.04634978 0.03485782
 0.04659636 0.11729529]
Feature ranking:

 1) 渠道1时长                          0.153462
 2) 渠道1消费                          0.141306
 3) 与客服沟通次数                      0.117295
 4) 渠道2消费                          0.083238
 5) 渠道2时长                          0.080326
 6) 渠道3时长                          0.055691
 7) 渠道3消费                         0.054776
 8) 渠道3访问次数                      0.048012
 9) 渠道1访问次数                      0.047125
10) 渠道4消费                          0.046596
11) 渠道4时长                          0.046350
12) 业务1使用次数                       0.045554
13) 渠道2访问次数                       0.045412
14) 渠道4访问次数                       0.034858

### 图形运行结果

![天纵之圣](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1597074450316&di=0c2f43b6daafb46c802fe24d1a5c004b&imgtype=0&src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fq_70%2Cc_zoom%2Cw_640%2Fimages%2F20180614%2Fe581a3ba8d7c429aa296980cef8fe7ff.jpeg)
